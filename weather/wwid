import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns



def stratified_kfold_with_groups(X, y, individual_id, n_splits=4):
    """
    Stratified K-fold cross-validation that respects the panel structure of the data.
    Ensures that all observations for the same individual are placed either in the training or testing set.

    Parameters:
    - X: DataFrame or ndarray containing the independent variables.
    - y: Series or ndarray containing the binary dependent variable (target).
    - individual_id: Series containing unique individual identifiers (one per individual).
    - n_splits: Integer, the number of splits/folds in the cross-validation. Default is 4.

    Returns:
    - train_idx, test_idx: Indices for training and testing splits respecting the individual_id groups.
    """
    # Create a StratifiedKFold object to split the data while preserving the distribution of the binary variable
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

    # Ensure individuals are either in train or test, not both
    for train_idx, test_idx in skf.split(X, y):
        # Get the unique individuals in each fold's train and test indices
        train_individuals = individual_id.iloc[train_idx].unique()
        test_individuals = individual_id.iloc[test_idx].unique()

        # If the same individual exists in both train and test, skip this split
        if any(individual_id.isin(train_individuals) & individual_id.isin(test_individuals)):
            continue
        
        # If no overlap of individuals, return the train and test indices
        yield train_idx, test_idx




# Function to calculate Gini coefficient from AUC score
def gini_auc(y_true, y_pred):
    auc = roc_auc_score(y_true, y_pred)
    return 2 * auc - 1  # Gini is 2 * AUC - 1

# Perform Logistic Regression with k-fold cross-validation
def kfold_logistic_regression(X, y, individual_id, n_splits=4):
    train_ginis = []
    test_ginis = []

    # Stratified K-fold with groups
    for X_train, X_test, y_train, y_test in stratified_kfold_with_groups(X, y, individual_id, n_splits):
        # Fit logistic regression model using sm.Logit
        model = sm.Logit(y_train, sm.add_constant(X_train))  # Adding constant for intercept
        result = model.fit()

        # Predictions on training and testing sets
        y_train_pred = result.predict(sm.add_constant(X_train))  # Predict on training set
        y_test_pred = result.predict(sm.add_constant(X_test))  # Predict on testing set

        # Calculate Gini scores for both training and testing datasets
        train_gini = gini_auc(y_train, y_train_pred)
        test_gini = gini_auc(y_test, y_test_pred)

        # Append the results
        train_ginis.append(train_gini)
        test_ginis.append(test_gini)

    return train_ginis, test_ginis



# Example: Assuming your data is loaded in `X`, `y`, and `individual_id`
# Run K-fold cross-validation and logistic regression
train_ginis, test_ginis = kfold_logistic_regression(X, y, individual_id, n_splits=4)

# Plotting the Gini distributions
plt.figure(figsize=(10, 6))
sns.histplot(train_ginis, kde=True, label='Train Gini', color='blue', stat="density", linewidth=0)
sns.histplot(test_ginis, kde=True, label='Test Gini', color='red', stat="density", linewidth=0)
plt.title("Distribution of Gini Coefficients (Train vs Test)")
plt.xlabel("Gini Coefficient")
plt.ylabel("Density")
plt.legend()
plt.show()

